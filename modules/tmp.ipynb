{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類似記事をはてぶから抜き出してみる\n",
    "協調フィルタリングだ！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def extract_urls(root_url):\n",
    "    \"\"\"\n",
    "    トップページを指定すると、ブログ内に存在するurlをすべて抜き出してくれる\n",
    "    \"\"\"\n",
    "    is_articles = True\n",
    "    page = 1\n",
    "    urls = []\n",
    "    # writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "    while is_articles:\n",
    "        try:\n",
    "            html = request.urlopen(\"{}/archive?page={}\".format(root_url, page))\n",
    "        except urllib.error.HTTPError as e: \n",
    "            # HTTPレスポンスのステータスコードが404, 403, 401などの例外処理\n",
    "            print(e.reason)\n",
    "            break\n",
    "        except urllib.error.URLError as e: \n",
    "            # アクセスしようとしたurlが無効なときの例外処理\n",
    "            print(e.reason)\n",
    "            break\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        articles = soup.find_all(\"a\",class_=\"entry-title-link\")\n",
    "        for article in articles:\n",
    "            urls.append(article.get(\"href\"))\n",
    "        if len(articles) == 0:\n",
    "            # articleがなくなったら終了\n",
    "            is_articles = False\n",
    "        page += 1\n",
    "    return urls\n",
    "\n",
    "root_url = \"http://www.procrasist.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.procrasist.com/entry/foreign-manga\n"
     ]
    }
   ],
   "source": [
    "urls = extract_urls(root_url)\n",
    "print(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！ほけきよです。\n",
      "ビットコインとか、流行ってません？\n",
      "流行っているので、私も気になるのです。\n",
      "とはいえ、仮想通貨とビットコインの違いすらわかっていないし、そもそも口座？みたいなのも持っていないので、\n",
      "このかきぴー氏((id:bbc-21)のサイトで勉強中というわけです。\n",
      "www.zerokarabitcoin.com\n",
      "だいぶバブリーな状況で、波に乗り遅れた感がスゴいですが、せっかくだしやってみようと思っています。\n",
      "それで、やっぱり勝ちたいじゃないですか？ 勝ちたいから分析したいじゃないですか？ 分析するにはデータがいるじゃないですか？\n",
      "見つけたんですよ、データ\n",
      "coinbin.org\n",
      "Pythonで使ってみる\n",
      "人気上位10銘柄の動きと相関\n",
      "お金持ちになりたい\n",
      "coinbin.org\n",
      "たまたまTLに流れてきたこれ、\n",
      "Coinbin.org: A Human–Friendly API Service for Crypto Currency Information\n",
      "最近流行ってるし、ちょっと見てみようとおもったら、結構良さそう\n",
      "特徴は下記の通り\n",
      "仮想通貨のその時点の価格をUSDとBTCの単位で取れるAPI\n",
      "流通してる858種類全てのデータが取れる\n",
      "返却はJSON形式だから扱いやすい\n",
      "データは5分おきに更新\n",
      "なかなか良さそうだな？とりあえず使ってみよう！！\n",
      "Pythonで使ってみる\n",
      "APIなので、request投げるとよしなに返してくれるでしょう。コードはこんな感じ\n",
      "これで、返される結果はこんなの。辞書形式で返ってきます。\n",
      "シンプルですけど、基本usdだけわかれば良いわけですし、いいですね。\n",
      "となって、少しだけ集めてみました。どんなことが出来るか、応用として参考にしてみてください。\n",
      "人気上位10銘柄の動きと相関\n",
      "人気銘柄を10分おきに8時間、データを集めてみました！グラフがコチラ*1\n",
      "\n",
      "結構人気銘柄は、それぞれ相関ありそうですな！\n",
      "ついでなので、遊び程度に相関係数も出してみた。\n",
      "\n",
      "こうやって数値化出来ると、いろいろと考察も捗るんじゃないでしょうか？\n",
      "ちなみに、このAPIは全銘柄の値を取れるので、せっかくだし全銘柄の相関係数も出してみた\n",
      "\n",
      "ちょっと細かすぎるけど、なんかしら使い道あるでしょう。\n",
      "お金持ちになりたい\n",
      "お金持ちになりたいので、絶賛データ収集中です。\n",
      "なんかしらのアルゴリズムにのせて、予測モデル構築したいなと。\n",
      "しかし、はじめにも言いましたがビットコインの知識が無なので、だれか助けてください。\n",
      "*1:値は全部0-1に収まるように正規化してます\n"
     ]
    }
   ],
   "source": [
    "url = urls[4]\n",
    "try:\n",
    "    html = request.urlopen(url)\n",
    "except urllib.error.HTTPError as e: \n",
    "    # HTTPレスポンスのステータスコードが404, 403, 401などの例外処理\n",
    "    print(e.reason)\n",
    "except urllib.error.URLError as e: \n",
    "    # アクセスしようとしたurlが無効なときの例外処理\n",
    "    print(e.reason)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "entry = soup.find(\"div\",class_=\"entry-content\")\n",
    "ps = entry.find_all([\"p\",\"h2\",\"h3\",\"li\"])\n",
    "for p in ps:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
